[ { "title": "üè•üë©üèΩ‚Äç‚öïÔ∏è Data Science Course Capstone Project - Healthcare domain - Diabetes Detection", "url": "/posts/Data-Science-Capstone-Project-Diabetes-Detection/", "categories": "", "tags": "Machine learning, Python, visualization, tableau, Data-science, Random-Forest, Exploratory Data Analysis, Classification, Knn-Classification, Logistic Regression, Random-Forest-Classifier, SVM, Naive-bayes, Imbalance-Classification, Xgboost-Classifier, Smote-Sampling, Classification-report, Precision-Recall-f1, sci-kit learn, numpy, pandas, matplotlib, seaborn", "date": "2022-01-04 17:53:00 +0530", "snippet": "Summary-This is comprehensive project completed by me as part of the Data Science Post Graduate Programme.This project includes multiple classification algorithms over a dataset collected on health/diagnostic variables to predict of a person has diabetes or not based on the data points. Apart from extensive EDA to understand the distribution and other aspects of the data, pre-processing was done to identify data which was missing or did not make sense within certain columns and imputation techniques were deployed to treat missing values. For classification the balance of classes was also reviewed and treated using SMOTE.Finally models were built using various classification algorithms and compared for accuracy on various metrics.Lastly the project contains a dashboard on the original data using Tableau.You can view the full project code on this Github linkNote: This is an academic project completed by me as part of my Post Graduate program in Data Science from Purdue University through Simplilearn. This project was towards final course completion.Bussiness ScenarioThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.ObjectiveBuild a model to accurately predict whether the patients in the dataset have diabetes or not.Analysis Steps Data Cleaning and Exploratory Data Analysis - Perform descriptive analysis. It is very important to understand the variables and corresponding values. Can minimum value of below listed columns be zero (0)? On these columns, a value of zero does not make sense and thus indicates missing value. Glucose BloodPressure SkinThickness Insulin BMI In this section we load the data and use the describe function of pandas for dataframes to view the quick snapshot of the statistical measures. There are no Nan values in the 768 rows. However as mentioned before we analyse the listed columns for 0 values and note that there are missing i.e.(0) values in all the columns. But for 2 of the columns SkinThickness and Insulin there is a high proportion of missing values. Decide how to treat the zero values in these columns - For imputation we need to observe the distribution of the variables to identify if mean or median imputation would be appropriate. Visually explore these variables and look for the distribution of these variables using histograms. Treat the missing values accordingly. We review the skewness of the distributions to decide if the imputation should be using mean or median. If the distribution has less skewness mean imputation can be done. For skewed data the median is more appropriate. Using the rule we have applied mean imputation only for BloodPressure. For the remaining 4 columns median imputation was done for the 0 values. There are integer as well as float data-type of variables in this dataset. Create a count (frequency) plot describing the data types and the count of variables. Check the balance of the data (to review imbalanced classes for the classification problem) by plotting the count of outcomes by their value. Review findings and plan future course of actions. We notice that there is class imbalance . The diabetic class (1) is the minority class and there are 35% samples for this class. However for the non-diabetic class(0) there are 65% of the total samples present. We need to balance the data using any oversampling for minority class or undersampling for majority class. This would help to ensure the model is balanced across both classes.We can apply the SMOTE (synthetic minority oversampling technique) method for balancing the samples by oversampling the minority class (class 1 - diabetic) as we would want to ensure model more accurately predicts when an individual has diabetes in our problem. Create scatter charts between the pair of variables to understand the relationships. Describe findings. We review scatter charts for analysing inter-relations between the variables and observe the following Most features do not show any strong correlation or patterns from the scatter plots. Only the following feature-pairs (BMI and SkinThickness) and (Glucose and Insulin) show some level of positive correlation Class seperation(Diabetic vs non diabetic) looks better in pairs containing ‚ÄúGlucose‚Äù as a feature. This could imply that the feature would contribute more to the classification and be a significant part of the model. Perform correlation analysis. Visually explore it using a heat map. Observation : As mentioned in the pairplot analysis the variable Glucose has the highest correlation to outcome. Model Building Devise strategies for model building. It is important to decide the right validation framework. Would Cross validation be useful in this scenario? A. Guideline/Main Approach For this problem, as it is true in the case of most other medical applications of classification algorithms we would want a model to ensure that we are able to correctly identify the cases where persons have the disease correctly. So based on this we concluded the worst case scenario to be a false negative. üî¥ False negative ‚Äì A person we told is not diabetic, but is actually having diabetes This makes for a worse situation as we gave a false signal to a person who requires treatment. If left untreated or untested this may lead to more health complications and severe consequences So based on the framework for this problem our aim should be to have a model where we are able to reduce the false-negatives as much as possible making the recall-score(sensitivity a more important factor) when making decisions for model selection B. Model building Strategy and Approach We will follow the below steps in building and model selection for the problem : First we perform resampling using SMOTE so that the classes(diabetic - 1 and non-diabetic - 0) are balanced in the data. Next we will split the data into Train-test(validation) split. The split will be done with a stratified sampling method to ensure that the splits retain the class distribution or proportion of samples for both classes. This step is done first to ensure there is no data leakage on the following steps where we will test models again using the validation data-set to decide the optimum model. Before we proceed with the model building. We would need to ensure that data is scaled uniformly. We will perform standard scaling as a final pre-processing step to bring all the features to same scale which is advisable for applying distance based algorithms like KNN. Next we shall proceed with model building. First we will build a model on KNN algorithm as a baseline with the best value of n_neighbors through GridSearchCV. Next we can run the K-Fold cross validation using the training data for the various other models to be tested on the classification problem. This should give us a idea of the overall performance of the models on multiple samples picked randomly from the train data. Finally we can again test the models on the validation data to analyse the classification model(s) with the best possible accuracy and recall. Model scores will be compared with scores from the KNN alogrithm identified in step 4. Lastly the final model is selected based on the validation criteria (i.e. best accuracy and recall) on test data along with good results on the train data during k-fold cross validation . Model also needs to be evaluated using various other metrics like f-1 score, confusion matrix and classification summary with auc score(roc curve), sensitivity and specificity metrics Apply an appropriate classification algorithm to build a model. Compare various models with the results from KNN. For KNN model the best value of n_neigbors = 17 based on GridSearch_CV which resulted in best combined score (i.e. best accuracy and recall scores). Random Forest Model has the best Combined score on Validation(test) data and this result is consistent with the results from K-fold cross validation on train data. XGBoost was second best on combined score, although we see XGB actually has better recall but Random forest had better accuracy overall leading to the slightly better combined score. The next 2 best performing models were SVM which is 4th on the and KNN has the 3rd best combined score on the (test)validation data due to it getting better recall than SVM. So overall we can see that Random forest gives us the best mix of accuracy with recall. We will shortlist the following 3 models Random Forest, XGBoost and K-Nearest Neighbors Classifier for the next step of analysis before we finalize the optimal model. Create a classification report by analysing sensitivity, specificity, AUC(ROC curve) etc. Decide what values of these parameter to settle for? any why? We tested the models using confusion matrix, classification report and finally AUC(ROC) curve. Model AUC Sensitivity Specificity RF 94.260 92.0 85.0 XGB 93.010 94.0 78.0 KNN 89.555 89.0 74.0 Note: ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two. This is quantified in the AUC Score. Final Analysis Based on the classification report: As per the framework decided earlier the sensitivity score is important and we can consider 90% as a minimum benchmark. Once again we see as per the classification report, AUC score is best for Random forest model. This means the model is able to differentiate the best between the two classes in the data. And while it has a lower sensitivity score than XGB model. It has a much better specificity score. Further note that the sensitivity score is not significantly lower for Random Forest when compared to XGBoost model. Both models clear the benchmark level of 90% on sensitivity. KNN scored lower than both Random Forest and XGBoost model overall. Based on these metrics we can select Random forest model to be the optimal model for this problem. Data Reporting Dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following: Pie chart to describe the diabetic/non-diabetic population Scatter charts between relevant variables to analyse the relationships Histogram/frequency charts to analyse the distribution of the data Heatmap of correlation analysis among the relevant variables Create bins of Age values ‚Äì 20-25, 25-30, 30-35 etc. and analyse different variables for these age brackets using a bubble chart. To view the tableau visualization visit this Tableau public page link Tools used:This project was completed in Python using Jupyter notebooks.Common libraries used for analysis include numpy, pandas, sci-kit learn, matplotlib, seaborn, xgboost" }, { "title": "üí¨‚öôÔ∏è NLP Project - Phone Review Analysis and Topic Modeling with Latent Dirichlet Allocation in Python", "url": "/posts/NLP-Project-Phone-Review-Analysis-and-Topic-Modelling-with-Latent-Dirichlet-Allocation-copy/", "categories": "", "tags": "Machine learning, Python, Natural Language Processing, Topic Modeling, Latent Dirchlet Allocation, POS-Tagging, numpy, pandas, nltk, gensim, matplotlib, pyLDAvis", "date": "2021-12-20 23:21:00 +0530", "snippet": "Summary-This is a Natural Language Processing project which includes analysis of buyer‚Äôs reviews/comments of a popular mobile phone by Lenovo from an e-commerce website. Analysis done for the project include pre-processing of text data such as word-tokenisation, lemmatisation. This is followed by Topic-modeling using Latent Dirichlet Allocation, POS tagging, and topic interpretation for business useYou can view the full project code on this Github linkNote: This is an academic project completed by me as part of my Post Graduate program in Data Science from Purdue University through Simplilearn. This project was towards the completion of Data Science Natural Language Processing (NLP) module.Bussiness ScenarioA popular mobile phone brand, Lenovo has launched their budget smartphone in the Indian market. The client wants to understand the VOC (voice of the customer) on the product. This will be useful to not just evaluate the current product, but to also get some direction for developing the product pipeline. The client is particularly interested in the different aspects that customers care about. Product reviews by customers on a leading e-commerce site should provide a good view.Analysis to be done: POS tagging, topic modeling using LDA, and topic interpretationObjectiveHelp the business understand the voice of the customer by analyzing the reviews of their product on Amazon and the topics that customers are talking about. Perform topic modeling on specific parts of speech. Finally interpret the emerging topics within the reviews for business.Analysis Steps Load the data - Read the .csv file using Pandas. Take a look at the top few records. Data Pre-processing : Before we proceed to perform our analysis we shall begin with some pre-procesing tasks to clean up the data and make it ready for topic modeling. We can make the text lower case first and then apply tokenization to break each individual review into seperate words. Following this we can perform Part-of-speech tagging, lemmatization and finally remove stopwords and punctuations which are not relevant to the topic modeling task. Normalize casings (convert to lower_case text) for the reviews and extract the text into a list for easier manipulation. Tokenize the reviews using NLTKs word_tokenize function. Perform parts-of-speech tagging on each sentence using the NLTK POS tagger. POS tagging helps us filter out text based on the part of speech that would be relevant to us. For example - When the requirement is to analyze names or places mentioned in a document, then we would want to specifically narrow our focus towards nouns within the text. OR if the requirement was to analyse the way people describe the subject of a text then we can tune the analysis to focus on the adjectives. For the topic model, we want to include only nouns.First we can find out all the POS tags that correspond to nouns and then limit the data to only terms with these tags. Lemmatization - Different forms of the terms need to be treated as one. Example - Very simply lemmatization reduces various forms of the same word to the base word. So eat, eating, eater, ate would be converted to the base word ‚Äúeat‚Äù across the multiple occurances within the text. Remove stopwords and punctuation (if there are any). Notes : After performing the pre-processing the number of reviews reduced from 14675 to 13453. We can still see some inconsistent items in the list of review words such as combined words like discharged.this and process.android.com. Also emoji‚Äôs are part of the text which are not relevant and need to be further removed. Next we can review the common words by creating a plot of the most common words.Following which we can do another round of stopwords and punctuation removal Next we shall visualize the list of words by creating a barplot of the most common/frequently occuring words in the reviews text. We first use FreqDist to find the most common 100 words. Then build the visualization in a barplot using matplotlib. The analysis reveals following - - There are some punctuations that still appear in the word tokens like ‚Äò..‚Äô and ‚Äò‚Ä¶‚Äô - Also the list consists of emojis which are not relevant for topic modeling - Further there are also just numbers - e.g. ‚Äò1‚Äô and ‚Äò2‚Äô which are not relevant - And we can also remove the obvious and contextual stop words from the topic modeling perspective . These are context Words like ‚Äòphone‚Äô,‚Äôlenovo‚Äô,‚Äômobile‚Äô,‚Äôk8‚Äô,‚Äôproduct‚Äô. Based on the above review we can refine and add more stopwords and items to be removed. The removal can be done using string functions like isalnum() which checks if a word token is alphanumeric. Also elminated tokens which are only a single alphabet by checking len(word)!=1.Through this pre-processing we have now narrowed down the reviews to the most relevant words/nouns in the text for the topic modeling phase. Again we have plotted the most common words after this second round of word filtering. Model Building : Create a topic model using LDA on the cleaned-up data with 12 topics. Print out the top terms for each topic. Check the coherence of the model with the c_v metric. Now we create the topic model using Latent Dirchlet Allocation algorithm. Latent Dirichlet Allocation (LDA) is a ‚Äúgenerative probabilistic model‚Äù of a collection of composites made up of parts. In terms of topic modeling, the composites are documents and the parts are words and/or phrases (n-grams). In simple terms LDA tries to create a model of what words appear in a particular manner or context together based on the frequency distributions and it aligns/classify the words into different topics. Furthermore it segments the entire corpus/document into these topics. Example : If we consider a news article . A given article could be based on only a few topics. Like an article in the sports section will largely focus on sports. At times the article may refer to some related topic like science if the article relates to some advances or findings in sports nutrition or excercise etc. But for the most part we can usually infer that a single article is mostly homogenous when it comes to the topics that it consists of. Again within a given topic like sports one can expect some words to be more frequent than others. E.g. In a sports article we are more like to see words like - ball, score, half-time, innings rather than words like data-science, javascript, python, C# , Machine learning. So the LDA algorithm performs this sorting or clustering of documents into a collection of topics and further it creates the topics by allocating words to the topics. The algorithm uses a frequency based (i.e. probabalistic approach).Like how many times does a particular word appear in the topic and how frequently is the topic part of the document. Based on this it creates a model of the document in an unsupervised fashion. The topics themselves like sports, politics, fashion are not named by the algorithm, this is up to the analyst to decide on the topic names based on the collection of words. So similar to clustering algorithms it creates clusters but it cannot actually describe what the specific category/cluster in the data is and such inference is drawn by manual/human review. The quality of the model can be determined using a metric called the c_v coherence metric. Coherence measures the relative distance between words within a topic. The overall coherence score of a topic is the average of the distances between words Next we analyze the topics through the business lens and determine which of the topics can be combined. While we started with the initial 12 topics. There may be a good amount of overlap between the topics. So we can review if some of the topics could be combined. Next we can create a refined topic model using LDA with the optimal number of topics as per previous analysis and check the coherence of the new model. Since the business should be able to interpret the topics per the objective of the project. Assign a relevant name for each of the identified topics. Create a table with the topic name and the top 10 terms in each topic to present to the business. We have also created a interactive visualization using pyLDAvis library to show the topics and the most popular terms within the topics Lastly we have created a simple wordcloud to vizualize the most popular and frequent words in the dataset. Tools used: This project was completed in Python using Jupyter notebooks.Common libraries used for analysis include numpy, pandas, nltk, gensim, matplotlib, pyLDAvis " }, { "title": "üîéüìä Principal Component Analysis with XGBoost Regression in Python", "url": "/posts/PCA-&-XGBoost-Regression-Python-Mercedes-Dataset/", "categories": "", "tags": "Machine learning, Python, Principal Component Analysis, Dimensionality Reduction, Label Encoding, numpy, pandas, sklearn, XGBoost Regression, matplotlib", "date": "2021-12-16 17:47:00 +0530", "snippet": "Summary-This project is based on data from the Mercedes Benz test bench for vehicles at the testing and quality assurance phase during the production cycle. The dataset consists of high number of feature columns. Key highlights from the project include - Dimensionality reduction using PCA and XGBoost Regression used after the dimensionality reduction to predict the time required to test the vehiclesYou can view the full project code on this Github linkNote: This is an academic project completed by me as part of my Post Graduate program in Data Science from Purdue University through Simplilearn. The project was towards the completion of Data Science Machine Learning module.Bussiness ScenarioSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include the passenger safety cell with a crumple zone, the airbag, and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium carmakers. Mercedes-Benz is the leader in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.To ensure the safety and reliability of every unique car configuration before they hit the road, the company‚Äôs engineers have developed a robust testing system. As one of the world‚Äôs biggest manufacturers of premium cars, safety and efficiency are paramount on Mercedes-Benz‚Äôs production lines. However, optimizing the speed of their testing system for many possible feature combinations is complex and time-consuming without a powerful algorithmic approach.We are required to reduce the time that cars spend on the test bench. Then we will work with a dataset representing different permutations of features in a Mercedes-Benz car to predict the time it takes to pass testing. Optimal algorithms will contribute to faster testing, resulting in lower carbon dioxide emissions without reducing Mercedes-Benz‚Äôs standards.ObjectiveReduce the time a Mercedes-Benz spends on the test bench.Analysis Steps Load the data : Load the train and test data csv files. Print first few rows of this data using head function. Review the shape of the data(i.e. No. of rows = Samples and No. of Columns = Features).Initially we notice the data consists of 377 feature columns. Data Cleaning : Check for any column(s),if the variance is equal to zero, then we can remove those variable(s)/feature(s). This can be done by reviewing if for any column the number of unique values is equal to 1. Then those columns are constant for all the samples in the data and would not have any impact in the prediction of the target variable ‚Äòy‚Äô Check for null values and unique values for both the train and test data. After removing the columns with no variance we are left with 365 columns/features from the original data. Data Pre-processing : There are some columns in the data set which are categorical. For performing PCA these need to be encoded to numerical form. We first find the relevant columns, then proceed to apply label encoding as some of these columns have many unique values which would make one-hot encoding undesirable for this problem as it would generate too many dummy variables. Dimensionality Reduction :For dimensionality reduction we will use the Principal Component Analysis technique. Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables. PCA is the most widely used tool in exploratory data analysis and in machine learning for predictive models. Moreover, PCA is an unsupervised statistical technique used to examine the interrelations among a set of variables. We use PCA implementation which is part of the sklearn decomposition package.Note: Before we apply PCA transformation for the data. We need to ensure the data is standardized. Hence we first apply the Standard scaling for the Train and test data and then proceed to apply the PCA transformation.Also PCA transformation by itself does not reduce the number of components. PCA actually expresses all the features/variables in the original data-set as principal components (linear combinations of original features based on the eigen decomposition). We can then select the number of such principal components to reduce the number of dimensions when building models or analysing the data further. Under PCA one can directly specify the number of components you want to extract OR we can mention the minimum amount of the variance of the original data which must be explained by the components.In this case we want to select the reduced number of components such that 95% of the original data variance is explained by the selected principal components. So when applying PCA transformation we set the n_components value to 0.95. After running the transformation we see that 149 principal components account for 95% of the information/variance from the original dataset. Model Building :Build a regression model on the data using XGBoost algorithm using the reduced/transformed version of the data after PCA with reduced features(principal components). The objective is to predict target variable ‚Äòy‚Äô which is the time taken to test the vehicles(i.e. time spent by the vehicle on the test bench). For the XGBoost Regressor we first try to find the best parameters on the Train data using GridSearchCV method. Next we have done K-fold cross validation using the best parameters model obtained by previous step. Finally we fit the model on the train data and check the overall accuracy. Then predict ‚Äòy‚Äô using the model on the test data. Note that since test data did not have the ‚Äòy‚Äô label. We cannot verify the accuracy of the model on the test data. Since we cannot verify the accuracy on test data, we have reviewed the model accuracy on the train data. Reviewed the actual v/s predicted values and same was plotted to visualize the quantity of deviation(error) from the actual y values. Tools used:This project was completed in Python using Jupyter notebooks.Common libraries used for analysis include numpy, pandas, matplotlib, sklearn and xgboost" }, { "title": "üè°üè∑Ô∏è California Housing Price Prediction using Linear Regression in Python", "url": "/posts/California-Housing-Price-Prediction/", "categories": "", "tags": "Machine learning, Python, Exploratory Data Analysis, Linear Regression, numpy, pandas, sklearn, seaborn, matplotlib", "date": "2021-12-12 12:37:00 +0530", "snippet": "Summary-The project includes analysis on the California Housing Dataset with some Exploratory data analysis .There was encoding of categorical data using the one-hot encoding present in pandas.Also standardization of the data and use of Linear Regression models from sklearn and Seaborn plotsYou can view the full project code on this Github linkNote: This is an academic project completed by me as part of my Post Graduate program in Data Science from Purdue University through Simplilearn. The project was towards the completion of Data Science with Python module.Bussiness ScenarioThe US Census Bureau has published California Census Data which has 10 types of metrics such as the population, median income, median housing price, and so on for each block group in California. The dataset also serves as an input for project scoping and tries to specify the functional and nonfunctional requirements for it. The project aims at building a model of housing prices to predict median house values in California using the provided dataset. This model should learn from the data and be able to predict the median housing price in any district, given all the other metrics. Districts or block groups are the smallest geographical units for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). There are 20,640 districts in the project dataset.Analysis Steps Load the data : Read the ‚Äúhousing.xlsx‚Äù file and print first few rows of the data. When we check the dataframe info we notice that there are missing values only for column total_bedrooms and on further review the missing value is for 207 rows in the data which is a smaller proportion of the total samples. Data-cleansing - Handle missing values : Missing value only for total_bedrooms which is a small portion of the total samples. So we can fill the missing values with the mean of the total_bedroom Data Pre-processing-I : Extract input (X) and output (Y) data from the dataset. The target column (y) is median_house_value and the remaining columns can be features. We have dropped longitude and latitude from the features as per the below note. Note: As longitude and latitude only define the block these columns are excluded from the features. As the objective is to create a linear regression model to predict housing prices or values in any district. So the location which is indicated by the block (in this data set the longitude and latitude variables) would not be relevant in the model building. Encode categorical data : Convert categorical column in the dataset to numerical data. The only categorical variable i.e (Ocean proximity) has 5 unique values. So we can perform a one-hot encoding for the different values of the variable. For this instance we are using the pandas get_dummies function and then dropping the original column. Exploratory Data Analysis : Perform some basic EDA to review the features and correlation Observations : There are no strong trends between median_house_value and any other feature variables other than median_income. Also a linear relation is seen only between ‚Äòhouseholds‚Äô and ‚Äòtotal_rooms‚Äô and ‚Äòtotal_bedrooms‚Äô which follows logically. Observations : From the correlation matrix we can see that median_house_value is more closely correlated to median_income in general. So this would be ideal feature to utilize in our regression model. As reviewed earlier from scatter plots - correlation is high between ‚Äòpopulation‚Äô, ‚Äòhouseholds‚Äô and ‚Äòtotal_rooms‚Äô and ‚Äòtotal_bedrooms‚Äô . Observation : We have also created a distribution/histogram plot of the target variable y (median_house_value). We note that the house prices have a certain level of skewness as there is a large number of samples where the house price is 500,000 which is comparable to the peak of the distribution around 160,000. Data Pre-processing-II : Split the dataset into 80% training dataset and 20% test dataset. Standardize training and test datasets. This will bring all the variables to the standard scale so the mean is 0 and variance is 1 after scaling. Perform Linear Regression : Perform Linear Regression on training data. Predict output for test dataset using the fitted model. Print root mean squared error (RMSE) from Linear Regression. ¬†¬†In Linear Regression models we try to create a linear equation also known as the line of best-fit through the data.Under simple linear regression we create a linear equation between the target (dependent) variable (y) and only one predictor (independent) variable (X) \\[ y= {Mx+C} \\] Under Multiple linear regression we form the linear equation between target(dependent) variable (y) and multiple predictor (independent) variables (\\(X_1, X_2, X_3\\)‚Ä¶so on) \\[ y={B_0+B_1X_1+B_2X_2+B_3X_3+‚Ä¶.+B_nX_n}\\] ¬†¬†Finally we can measure the model accuracy using various metrics. The most common ones being the R-squared, Adjusted R-squared and RMSE. \\[R^2 = {1-\\frac{SumOfSquares_{Residual}}{SumOfSquares_{Total}}} \\] \\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(x_i-y_i)^2}{N}}\\] Adjusted R-squared is a very standard metric in regression and helps to measure how much of the variation in the target variable(y) is explained by the independent predictor variables. The higher the score, the better the model is at predicting the target. RMSE in simple terms is the square root of the mean of squared deviations between actual and predicted values by the model. We have implemented regression using the sci-kit learn library. The metrics are also present within sci-kit learn. Note that there is also another package statsmodels which can be used for implementing regression in Python. The output also contains lots of metrics on the created model. Additional analysis : Perform Linear Regression with only one independent variable Extract just the median_income column from the independent variables (from X_train and X_test). Perform Linear Regression to predict housing values based on median_income. Predict output for test dataset using the fitted model. Plot the fitted model for training data as well as for test data to check if the fitted model satisfies the test data. Tools used:This project was completed in Python using Jupyter notebooks.Common libraries used for analysis include numpy, pandas, matplotlib, seaborn, sklearn" }, { "title": "üõçÔ∏èüõí Retail Supermarket Store Sales Analysis and Regression models in R", "url": "/posts/Retail-Supermarket-Store-Sales-Analysis/", "categories": "", "tags": "Machine learning, R, Exploratory Data Analysis, Linear Regression", "date": "2021-12-05 13:45:00 +0530", "snippet": "Summary-The project includes an analysis of a Giant retail supermarket chain‚Äôs sales data for a period of 2.5 years using R to build a regression model for forecasting Sales and demand.The project also includes exploratory analysis of the store data to review the effect of markdowns due to holidays on the sales. Do the markdowns and discounts help sales during the holiday season. Along with this there was also general insights about the sales across the various locations/outlets to identify stores with most sales and variation in sales across stores and across months etc.You can view the full project code on this Github linkNote: This is an academic project completed by me as part of my Post Graduate program in Data Science from Purdue University through Simplilearn. The project was towards the completion of Data Science with R module.Bussiness ScenarioOne of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc.Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.Analysis Steps Answer few questions based on Exploratory Data Analysis like: Find the store which has maximum sales and store with maximum standard deviation i.e., the sales vary a lot. Which store/s has good quarterly growth rate in Q3‚Äô2012 Some holidays have a negative impact on sales. Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together. In this section we used the lubridate package to extract the Quarter from the Date variable. The function to extract quarters also provides option to select the fiscal start date. Since the data starts from Feb-2010. We have checked the Quarterly growth under both situations. When selecting the fiscal start from the second month(Feb) and also when selecting fiscal start from the first month which is the default option. In both cases this causes the results to vary and the store with the sales best quarterly sales growth was a different store in each case. For the Holiday sales analysis we notice - Out of all the Holidays Thanksgiving has the max Average Holiday sales. We see that only for Christmas the Sales are below the Non holiday average. One must note that the total dollar-value of the sales is not a fair comparison between Non-Holiday v/s Holidays. Since Holidays normally have a markdown(discount). Due to the markdown and discounts during holiday the dollar($) amount (Price) of any given product is lesser. This means that while there could be a possibility that actual units sold were higher during holidays .The final dollar value of the Sales can actually be lesser or equal the Non discounted sales value during the normal period. So this comparison is dependent upon the amount of Holiday discount which is not specified in this analysis/dataset. Creating a monthly and semester view of sales in units and insights on the same. Monthly sales Insights : The average sales are highest in last part (i.e. months 11 and 12) of the year. January has lowest average sales (923884.6) and December average sales(1281863.6) is the highest. Also note that December has a higher Standard Deviation in the sales. The box plot also confirms this as there are plenty of outlier points on the higher side of the scale in December. Also there appears to be a dip in sales during September and October where the average sales is below 1000000. Semester sales Insights : On Average the 2nd Semester sales are higher than 1st Semester. Except for in the year 2012. This is logically conclusive as well since the monthly analysis validates that sales on average is higher in Nov and Dec months which fall in 2nd Semester. Note that for 2012- 2nd Semester data is incomplete/less as data was available only till October 2012. And as noted in monthly analysis the November and December month average sales tend to be the highest of all months in the year. Again we see a high variance of weekly sales in the 2nd Semester with a large number of outliers. This again follows from our conclusion in Monthly sales analysis which revealed high number of outliers for weekly sales figures in Nov and Dec. Model Building: Linear Regression models built and analysed for features leading to best possible accuracy and to ensure models contain features that are statistically significant. Note: Model was created for only store #1 specifically under this project as per the stated requirement. For the store there was only 143 samples. It would be advisable to have more data per store if we want to specifically focus on creating a model for one store. Else a general model could be created by ingesting the data across stores using the other predictive feature variables First build a general model for demand - with all of the variables first and then test various methods to improve models. We have then proceeded to analyse features to identify features which are statistically significant by reviewing the p-values for each variable. Using a base level of significance at 5% for our test. We eliminate the variables where p-value was more than the significance threshold of 0.05 and refine the model successively. At each revision we review the adjusted R-squared as well which shows how well the model explains the target variable(sales) based on the predictor variables. Next revised the Model using Akaike information criterion (AIC) Statistical algorithm. Using this we obtained a model with adjusted R-squared of 0.3384. But even this is very low to be considered for deployment by bussiness for the forecasting sales. Next we have analysed the models for multicollinearity between the variables using VIF. Note that the categorical variables have no VIF data. Run the VIF step algorithm and it eliminates variables which have a collinearity problem using VIF threshold of 5. This results in a final model with only Temperature and CPI. But this model has a very low adjusted R-squared of 0.1012. Again this model does not have any categorical variables as VIF cannot be applied to categorical features. However as per the previous step the best model we could build had a maximum adjusted R-squared of 0.3384. To conclude the data is insufficient/less or we need other features which would be more significant in terms of predicting the target variable(weekly_sales). Hypothesize if CPI, unemployment, and fuel price have any impact on sales. ¬†¬†¬†¬†¬†Our review of the hypothesis revealed that out of CPI, unemployment, and fuel price. Fuel price does not have a significant impact or effect on the target variable (Weekly_Sales). CPI and Unemployment have statistical significance and their coefficients have a low p-value. However with the model having a very low adjusted R-squared (0.06524) this means they can explain very little of the variation in weekly sales. So while they do have an impact on sales however the impact is very weak or low and cannot successfully help predict the target variable. Conclusions and select the model which gives best accuracy. ¬†¬†¬†¬†¬†Overall none the models result in sufficient scores to qualify for being used to forecast weekly_sales. However as mentioned before this could be due to the model being narrowed down to only one store which reduces the number of samples for training significantly. While it does make sense that predictions would vary from store to store based on region, location and store-size being some basic factors causing variances, yet we also need to explore a general model independent of store or have sufficient data for each store to train a specific model for the store. Also bussiness needs to explore other features which may better predict sales.Tools used:This project was completed in R programming lanuage and used various statistical packages within R like dplyr, lubridate, sp, raster, usdm" }, { "title": "üë®‚Äçüíª ABOUT ME", "url": "/posts/ABOUT-ME/", "categories": "", "tags": "about me", "date": "2021-12-01 12:00:00 +0530", "snippet": "Aspiring data science professional with experience in data analysis, dashboarding and visualisation.Hello world,I am primarily a Banking professional with over 7 years of experience in banking (credit operations, loan processing) and data analysis roles.I am experienced in Data Analysis, MIS Reporting and Dashboards using tools like SQL, Excel and Tableau.I know Python and R programming languages.I have also completed a Post Graduate Certification in Data Science from Purdue University through Simplilearn.As part of the certification I have worked on various projects and learnt various fundamental skills for data science and machine learning. This includes working with various python libraries such as pandas, sci-kit learn, matplotlib, seaborn , nltk and gensim for natural language processing.Skills üíª Data Science and Machine Learning Data Analysis Dashboarding and Visualization Natural Language ProcessingTools üõ†Ô∏è ¬†¬†¬†¬†¬†¬†¬†¬†Python ¬†¬†¬†¬†¬†¬†¬†¬†R ¬†¬†¬†¬†¬†¬†¬†¬†SQL ¬†¬†¬†¬†¬†¬†¬†¬†TableauCertifications üë®üèª‚ÄçüéìPGP in Data Science - Purdue UniversityResumeClick here to see my Resume" } ]
